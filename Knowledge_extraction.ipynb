{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in c:\\users\\ramya\\anaconda3\\lib\\site-packages (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            File Name  Page Number  \\\n",
      "0                                    2305.05422v1.pdf            1   \n",
      "1                                    2305.05422v1.pdf            2   \n",
      "2                                    2305.05422v1.pdf            3   \n",
      "3                                    2305.05422v1.pdf            4   \n",
      "4                                    2305.05422v1.pdf            5   \n",
      "5                                    2305.05422v1.pdf            6   \n",
      "6                                    2305.05422v1.pdf            7   \n",
      "7                                    2305.05422v1.pdf            8   \n",
      "8                                    2305.05422v1.pdf            9   \n",
      "9                                    2305.05422v1.pdf           10   \n",
      "10  A_Conceptual_Model_for_Implementing_Explainabl...            1   \n",
      "11  A_Conceptual_Model_for_Implementing_Explainabl...            2   \n",
      "12  A_Conceptual_Model_for_Implementing_Explainabl...            3   \n",
      "13  A_Conceptual_Model_for_Implementing_Explainabl...            4   \n",
      "14  A_Conceptual_Model_for_Implementing_Explainabl...            5   \n",
      "15  A_Conceptual_Model_for_Implementing_Explainabl...            6   \n",
      "16  A_Conceptual_Model_for_Implementing_Explainabl...            7   \n",
      "17  A_Conceptual_Model_for_Implementing_Explainabl...            8   \n",
      "18  A_Conceptual_Model_for_Implementing_Explainabl...            9   \n",
      "19  A_Conceptual_Model_for_Implementing_Explainabl...           10   \n",
      "20  A_Conceptual_Model_for_Implementing_Explainabl...           11   \n",
      "21  A_Conceptual_Model_for_Implementing_Explainabl...           12   \n",
      "22  A_Conceptual_Model_for_Implementing_Explainabl...           13   \n",
      "23  A_Conceptual_Model_for_Implementing_Explainabl...           14   \n",
      "24                             hhai-2022_paper_67.pdf            1   \n",
      "25                             hhai-2022_paper_67.pdf            2   \n",
      "26                             hhai-2022_paper_67.pdf            3   \n",
      "27                             hhai-2022_paper_67.pdf            4   \n",
      "28                             hhai-2022_paper_67.pdf            5   \n",
      "29                             hhai-2022_paper_67.pdf            6   \n",
      "30                             hhai-2022_paper_67.pdf            7   \n",
      "31                             hhai-2022_paper_67.pdf            8   \n",
      "32  Legal-Ethical_Challenges_and_Technological_Sol...            1   \n",
      "33  Legal-Ethical_Challenges_and_Technological_Sol...            2   \n",
      "34  Legal-Ethical_Challenges_and_Technological_Sol...            3   \n",
      "35  Legal-Ethical_Challenges_and_Technological_Sol...            4   \n",
      "36  Legal-Ethical_Challenges_and_Technological_Sol...            5   \n",
      "37  Legal-Ethical_Challenges_and_Technological_Sol...            6   \n",
      "38  Legal-Ethical_Challenges_and_Technological_Sol...            7   \n",
      "39  Legal-Ethical_Challenges_and_Technological_Sol...            8   \n",
      "40  Legal-Ethical_Challenges_and_Technological_Sol...            9   \n",
      "41  Legal-Ethical_Challenges_and_Technological_Sol...           10   \n",
      "42  Legal-Ethical_Challenges_and_Technological_Sol...           11   \n",
      "43  Open_Multiple_Adjunct_Decision_Support_at_the_...            1   \n",
      "44  Open_Multiple_Adjunct_Decision_Support_at_the_...            2   \n",
      "45  Open_Multiple_Adjunct_Decision_Support_at_the_...            3   \n",
      "\n",
      "                                                 Text  \n",
      "0   L. Erculiani et al. /\\nEgocentric Hierarchical...  \n",
      "1   L. Erculiani et al. /\\nMusical instrumentsG: d...  \n",
      "2   L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...  \n",
      "3   L. Erculiani et al. /\\nAlgorithm 1The main loo...  \n",
      "4   L. Erculiani et al. /\\nAlgorithm 2The procedur...  \n",
      "5   L. Erculiani et al. /\\ncurrentGenus candidateG...  \n",
      "6   L. Erculiani et al. /\\nFigure 5. Comparison be...  \n",
      "7   L. Erculiani et al. /\\n5. Related work\\nOur wo...  \n",
      "8   L. Erculiani et al. /\\nReferences\\n[1] Kuang Z...  \n",
      "9   L. Erculiani et al. /\\n[28] Fu Y , Dong H, Ma ...  \n",
      "10    \\nA Conceptual Model for Implementing \\nExpl...  \n",
      "11    \\ntechniques, such as neural networks, AI mo...  \n",
      "12    \\n2.1. Application of XAI in General. \\nThe ...  \n",
      "13    \\nThe following terms are used in Table 1 an...  \n",
      "14    \\nnext step, the interviewee was asked to re...  \n",
      "15    \\n3.2. Validation of Draft Conceptual Model ...  \n",
      "16    \\nFigure 1. Conceptual model of categories o...  \n",
      "17    \\non how the explanation should be given’. I...  \n",
      "18    \\ngreater the need for XAI. P12 indicates: ‘...  \n",
      "19    \\nwhat data is used and how, reasons for a s...  \n",
      "20    \\n4.2. Validation of Conceptual Model \\nTwo ...  \n",
      "21    \\nThe strength of the conceptual model is it...  \n",
      "22    \\nReferences \\n[1] Benbya H, Davenport TH, P...  \n",
      "23    \\n[24] Phillips PJ, Hahn CA, Fontana PC, Bro...  \n",
      "24  Knowledge Graphs in support of\\nHuman-Machine ...  \n",
      "25  • An example use-case to ground our discussion...  \n",
      "26  Figure 1. State of the art approaches to makin...  \n",
      "27  Considering this definition Figure 2 represent...  \n",
      "28  Paraphrasing the research agenda for Hybrid In...  \n",
      "29  4. Proposal for an hybrid approach\\nAs outline...  \n",
      "30  As outlined earlier, Figure 3 keeps this focus...  \n",
      "31  E. Sirin, T. Tudorache, J. Euzenat, M. Hauswir...  \n",
      "32  Legal-Ethical Challenges and\\nTechnological So...  \n",
      "33  exclusively medical context, ethics’ interplay...  \n",
      "34  Unlike more rigid consent practices in medicin...  \n",
      "35  For consent to be valid, the data subject shou...  \n",
      "36  CMPs can more transparently communicate proces...  \n",
      "37  4.1. Technical Standards, Ontologies, and Mech...  \n",
      "38  as consent preferences, thereby enabling perso...  \n",
      "39  of consent and digital platform centering data...  \n",
      "40  References\\n[1] Ryan KJ, Brady JV , Cooke RE, ...  \n",
      "41  [26] for German Supervisory Authorities A. Gui...  \n",
      "42  Data Flows to Facilitate Compliance V eriﬁcati...  \n",
      "43  Open, Multiple, Adjunct. Decision Support\\nat ...  \n",
      "44  A vague system, as in the case of multiplicity...  \n",
      "45  References\\n[1] De Michelis G. Aperto, moltepl...  \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    \"\"\"\n",
    "    Extracts text from all PDFs in a given folder and returns a Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        pdf_folder (str): Path to the folder containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing extracted text with columns: \n",
    "                      [\"File Name\", \"Page Number\", \"Text\"]\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    # List all PDF files in the given folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        reader = PdfReader(pdf_path)\n",
    "\n",
    "        # Extract text from each page\n",
    "        for page_number, page in enumerate(reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:  # Avoid empty pages\n",
    "                all_data.append([pdf_file, page_number + 1, text])\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=[\"File Name\", \"Page Number\", \"Text\"])\n",
    "    return df\n",
    "\n",
    "# Example Usage:\n",
    "pdf_folder = \"research_papers\"  # Replace with the actual folder path\n",
    "df = extract_text_from_pdfs(pdf_folder)\n",
    "\n",
    "# Display the extracted text\n",
    "df.to_csv(\"extracted_text.csv\", index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>L. Erculiani et al. /\\nEgocentric Hierarchical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>L. Erculiani et al. /\\nMusical instrumentsG: d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 1The main loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 2The procedur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>6</td>\n",
       "      <td>L. Erculiani et al. /\\ncurrentGenus candidateG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>7</td>\n",
       "      <td>L. Erculiani et al. /\\nFigure 5. Comparison be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>8</td>\n",
       "      <td>L. Erculiani et al. /\\n5. Related work\\nOur wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>9</td>\n",
       "      <td>L. Erculiani et al. /\\nReferences\\n[1] Kuang Z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>10</td>\n",
       "      <td>L. Erculiani et al. /\\n[28] Fu Y , Dong H, Ma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nA Conceptual Model for Implementing \\nExpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>2</td>\n",
       "      <td>\\ntechniques, such as neural networks, AI mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>3</td>\n",
       "      <td>\\n2.1. Application of XAI in General. \\nThe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>4</td>\n",
       "      <td>\\nThe following terms are used in Table 1 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>5</td>\n",
       "      <td>\\nnext step, the interviewee was asked to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>6</td>\n",
       "      <td>\\n3.2. Validation of Draft Conceptual Model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>7</td>\n",
       "      <td>\\nFigure 1. Conceptual model of categories o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>8</td>\n",
       "      <td>\\non how the explanation should be given’. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>9</td>\n",
       "      <td>\\ngreater the need for XAI. P12 indicates: ‘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A_Conceptual_Model_for_Implementing_Explainabl...</td>\n",
       "      <td>10</td>\n",
       "      <td>\\nwhat data is used and how, reasons for a s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            File Name  Page Number  \\\n",
       "0                                    2305.05422v1.pdf            1   \n",
       "1                                    2305.05422v1.pdf            2   \n",
       "2                                    2305.05422v1.pdf            3   \n",
       "3                                    2305.05422v1.pdf            4   \n",
       "4                                    2305.05422v1.pdf            5   \n",
       "5                                    2305.05422v1.pdf            6   \n",
       "6                                    2305.05422v1.pdf            7   \n",
       "7                                    2305.05422v1.pdf            8   \n",
       "8                                    2305.05422v1.pdf            9   \n",
       "9                                    2305.05422v1.pdf           10   \n",
       "10  A_Conceptual_Model_for_Implementing_Explainabl...            1   \n",
       "11  A_Conceptual_Model_for_Implementing_Explainabl...            2   \n",
       "12  A_Conceptual_Model_for_Implementing_Explainabl...            3   \n",
       "13  A_Conceptual_Model_for_Implementing_Explainabl...            4   \n",
       "14  A_Conceptual_Model_for_Implementing_Explainabl...            5   \n",
       "15  A_Conceptual_Model_for_Implementing_Explainabl...            6   \n",
       "16  A_Conceptual_Model_for_Implementing_Explainabl...            7   \n",
       "17  A_Conceptual_Model_for_Implementing_Explainabl...            8   \n",
       "18  A_Conceptual_Model_for_Implementing_Explainabl...            9   \n",
       "19  A_Conceptual_Model_for_Implementing_Explainabl...           10   \n",
       "\n",
       "                                                 Text  \n",
       "0   L. Erculiani et al. /\\nEgocentric Hierarchical...  \n",
       "1   L. Erculiani et al. /\\nMusical instrumentsG: d...  \n",
       "2   L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...  \n",
       "3   L. Erculiani et al. /\\nAlgorithm 1The main loo...  \n",
       "4   L. Erculiani et al. /\\nAlgorithm 2The procedur...  \n",
       "5   L. Erculiani et al. /\\ncurrentGenus candidateG...  \n",
       "6   L. Erculiani et al. /\\nFigure 5. Comparison be...  \n",
       "7   L. Erculiani et al. /\\n5. Related work\\nOur wo...  \n",
       "8   L. Erculiani et al. /\\nReferences\\n[1] Kuang Z...  \n",
       "9   L. Erculiani et al. /\\n[28] Fu Y , Dong H, Ma ...  \n",
       "10    \\nA Conceptual Model for Implementing \\nExpl...  \n",
       "11    \\ntechniques, such as neural networks, AI mo...  \n",
       "12    \\n2.1. Application of XAI in General. \\nThe ...  \n",
       "13    \\nThe following terms are used in Table 1 an...  \n",
       "14    \\nnext step, the interviewee was asked to re...  \n",
       "15    \\n3.2. Validation of Draft Conceptual Model ...  \n",
       "16    \\nFigure 1. Conceptual model of categories o...  \n",
       "17    \\non how the explanation should be given’. I...  \n",
       "18    \\ngreater the need for XAI. P12 indicates: ‘...  \n",
       "19    \\nwhat data is used and how, reasons for a s...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\ramya\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ramya\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Page Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.826087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.535875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Page Number\n",
       "count    46.000000\n",
       "mean      5.826087\n",
       "std       3.535875\n",
       "min       1.000000\n",
       "25%       3.000000\n",
       "50%       5.500000\n",
       "75%       8.000000\n",
       "max      14.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46 entries, 0 to 45\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   File Name    46 non-null     object\n",
      " 1   Page Number  46 non-null     int64 \n",
      " 2   Text         46 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File Name      0\n",
       "Page Number    0\n",
       "Text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceed with nlp processing \n",
    "import spacy \n",
    "import nltk # natural language package similar to spacy \n",
    "import string\n",
    "import re # regular expression\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowering the text in the data df\n",
    "df['Text_lowered'] = df['Text'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lowered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>L. Erculiani et al. /\\nEgocentric Hierarchical...</td>\n",
       "      <td>l. erculiani et al. /\\negocentric hierarchical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>L. Erculiani et al. /\\nMusical instrumentsG: d...</td>\n",
       "      <td>l. erculiani et al. /\\nmusical instrumentsg: d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...</td>\n",
       "      <td>l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 1The main loo...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 1the main loo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 2The procedur...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 2the procedur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          File Name  Page Number  \\\n",
       "0  2305.05422v1.pdf            1   \n",
       "1  2305.05422v1.pdf            2   \n",
       "2  2305.05422v1.pdf            3   \n",
       "3  2305.05422v1.pdf            4   \n",
       "4  2305.05422v1.pdf            5   \n",
       "\n",
       "                                                Text  \\\n",
       "0  L. Erculiani et al. /\\nEgocentric Hierarchical...   \n",
       "1  L. Erculiani et al. /\\nMusical instrumentsG: d...   \n",
       "2  L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...   \n",
       "3  L. Erculiani et al. /\\nAlgorithm 1The main loo...   \n",
       "4  L. Erculiani et al. /\\nAlgorithm 2The procedur...   \n",
       "\n",
       "                                        Text_lowered  \n",
       "0  l. erculiani et al. /\\negocentric hierarchical...  \n",
       "1  l. erculiani et al. /\\nmusical instrumentsg: d...  \n",
       "2  l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...  \n",
       "3  l. erculiani et al. /\\nalgorithm 1the main loo...  \n",
       "4  l. erculiani et al. /\\nalgorithm 2the procedur...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removal of unwanted charecters and punctuations\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_cleaned'] = df['Text_lowered'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lowered</th>\n",
       "      <th>Text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>L. Erculiani et al. /\\nEgocentric Hierarchical...</td>\n",
       "      <td>l. erculiani et al. /\\negocentric hierarchical...</td>\n",
       "      <td>l erculiani et al \\negocentric hierarchical vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>L. Erculiani et al. /\\nMusical instrumentsG: d...</td>\n",
       "      <td>l. erculiani et al. /\\nmusical instrumentsg: d...</td>\n",
       "      <td>l erculiani et al \\nmusical instrumentsg devic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...</td>\n",
       "      <td>l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...</td>\n",
       "      <td>l erculiani et al \\nv1\\n v2\\n v3\\ne\\nfigure 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 1The main loo...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 1the main loo...</td>\n",
       "      <td>l erculiani et al \\nalgorithm 1the main loop o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 2The procedur...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 2the procedur...</td>\n",
       "      <td>l erculiani et al \\nalgorithm 2the procedure p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          File Name  Page Number  \\\n",
       "0  2305.05422v1.pdf            1   \n",
       "1  2305.05422v1.pdf            2   \n",
       "2  2305.05422v1.pdf            3   \n",
       "3  2305.05422v1.pdf            4   \n",
       "4  2305.05422v1.pdf            5   \n",
       "\n",
       "                                                Text  \\\n",
       "0  L. Erculiani et al. /\\nEgocentric Hierarchical...   \n",
       "1  L. Erculiani et al. /\\nMusical instrumentsG: d...   \n",
       "2  L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...   \n",
       "3  L. Erculiani et al. /\\nAlgorithm 1The main loo...   \n",
       "4  L. Erculiani et al. /\\nAlgorithm 2The procedur...   \n",
       "\n",
       "                                        Text_lowered  \\\n",
       "0  l. erculiani et al. /\\negocentric hierarchical...   \n",
       "1  l. erculiani et al. /\\nmusical instrumentsg: d...   \n",
       "2  l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...   \n",
       "3  l. erculiani et al. /\\nalgorithm 1the main loo...   \n",
       "4  l. erculiani et al. /\\nalgorithm 2the procedur...   \n",
       "\n",
       "                                        Text_cleaned  \n",
       "0  l erculiani et al \\negocentric hierarchical vi...  \n",
       "1  l erculiani et al \\nmusical instrumentsg devic...  \n",
       "2  l erculiani et al \\nv1\\n v2\\n v3\\ne\\nfigure 2 ...  \n",
       "3  l erculiani et al \\nalgorithm 1the main loop o...  \n",
       "4  l erculiani et al \\nalgorithm 2the procedure p...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 53.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 44.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable={\"ner\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization of df\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text_lemmatized'] = df['Text_cleaned'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File Name</th>\n",
       "      <th>Page Number</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_lowered</th>\n",
       "      <th>Text_cleaned</th>\n",
       "      <th>Text_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>L. Erculiani et al. /\\nEgocentric Hierarchical...</td>\n",
       "      <td>l. erculiani et al. /\\negocentric hierarchical...</td>\n",
       "      <td>l erculiani et al \\negocentric hierarchical vi...</td>\n",
       "      <td>l erculiani et al \\n egocentric hierarchical v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>L. Erculiani et al. /\\nMusical instrumentsG: d...</td>\n",
       "      <td>l. erculiani et al. /\\nmusical instrumentsg: d...</td>\n",
       "      <td>l erculiani et al \\nmusical instrumentsg devic...</td>\n",
       "      <td>l erculiani et al \\n musical instrumentsg devi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...</td>\n",
       "      <td>l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...</td>\n",
       "      <td>l erculiani et al \\nv1\\n v2\\n v3\\ne\\nfigure 2 ...</td>\n",
       "      <td>l erculiani et al \\n v1 \\n  v2 \\n  v3 \\n e \\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 1The main loo...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 1the main loo...</td>\n",
       "      <td>l erculiani et al \\nalgorithm 1the main loop o...</td>\n",
       "      <td>l erculiani et al \\n algorithm 1the main loop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2305.05422v1.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>L. Erculiani et al. /\\nAlgorithm 2The procedur...</td>\n",
       "      <td>l. erculiani et al. /\\nalgorithm 2the procedur...</td>\n",
       "      <td>l erculiani et al \\nalgorithm 2the procedure p...</td>\n",
       "      <td>l erculiani et al \\n algorithm 2the procedure ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          File Name  Page Number  \\\n",
       "0  2305.05422v1.pdf            1   \n",
       "1  2305.05422v1.pdf            2   \n",
       "2  2305.05422v1.pdf            3   \n",
       "3  2305.05422v1.pdf            4   \n",
       "4  2305.05422v1.pdf            5   \n",
       "\n",
       "                                                Text  \\\n",
       "0  L. Erculiani et al. /\\nEgocentric Hierarchical...   \n",
       "1  L. Erculiani et al. /\\nMusical instrumentsG: d...   \n",
       "2  L. Erculiani et al. /\\nv1\\n v2\\n v3\\nE\\nFigure...   \n",
       "3  L. Erculiani et al. /\\nAlgorithm 1The main loo...   \n",
       "4  L. Erculiani et al. /\\nAlgorithm 2The procedur...   \n",
       "\n",
       "                                        Text_lowered  \\\n",
       "0  l. erculiani et al. /\\negocentric hierarchical...   \n",
       "1  l. erculiani et al. /\\nmusical instrumentsg: d...   \n",
       "2  l. erculiani et al. /\\nv1\\n v2\\n v3\\ne\\nfigure...   \n",
       "3  l. erculiani et al. /\\nalgorithm 1the main loo...   \n",
       "4  l. erculiani et al. /\\nalgorithm 2the procedur...   \n",
       "\n",
       "                                        Text_cleaned  \\\n",
       "0  l erculiani et al \\negocentric hierarchical vi...   \n",
       "1  l erculiani et al \\nmusical instrumentsg devic...   \n",
       "2  l erculiani et al \\nv1\\n v2\\n v3\\ne\\nfigure 2 ...   \n",
       "3  l erculiani et al \\nalgorithm 1the main loop o...   \n",
       "4  l erculiani et al \\nalgorithm 2the procedure p...   \n",
       "\n",
       "                                     Text_lemmatized  \n",
       "0  l erculiani et al \\n egocentric hierarchical v...  \n",
       "1  l erculiani et al \\n musical instrumentsg devi...  \n",
       "2  l erculiani et al \\n v1 \\n  v2 \\n  v3 \\n e \\n ...  \n",
       "3  l erculiani et al \\n algorithm 1the main loop ...  \n",
       "4  l erculiani et al \\n algorithm 2the procedure ...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ramya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ramya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ramya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")  # Sentence and word tokenization\n",
    "nltk.download(\"stopwords\")  # If you use stopwords\n",
    "nltk.download(\"wordnet\")  # If using lemmatization\n",
    "nltk.download(\"averaged_perceptron_tagger\")  # If using POS tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(r\"C:\\Users\\ramya\\OneDrive\\Desktop\\Ramya\\VU - Masters\\period_3\\Kg_group_assignment\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\ramya/nltk_data', 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\nltk_data', 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ramya\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'nltk_data', 'C:\\\\Users\\\\ramya\\\\OneDrive\\\\Desktop\\\\Ramya\\\\VU - Masters\\\\period_3\\\\Kg_group_assignment\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization of lemmatized text\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ramya/nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ramya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'nltk_data'\n    - 'C:\\\\Users\\\\ramya\\\\OneDrive\\\\Desktop\\\\Ramya\\\\VU - Masters\\\\period_3\\\\Kg_group_assignment\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize_text(x))\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText_lemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize_text(x))\n",
      "Cell \u001b[1;32mIn[90], line 4\u001b[0m, in \u001b[0;36mtokenize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_text\u001b[39m(text):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word_tokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\ramya\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ramya/nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ramya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ramya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'nltk_data'\n    - 'C:\\\\Users\\\\ramya\\\\OneDrive\\\\Desktop\\\\Ramya\\\\VU - Masters\\\\period_3\\\\Kg_group_assignment\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df['Text_tokens'] = df['Text_lemmatized'].apply(lambda x: tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords from tokens\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
